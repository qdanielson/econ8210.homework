---
title: "ECON8210 HW1"
author: "Quinn Danielson"
date: "2024-11-01"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

#Load Rccp for fast looping
library(Rcpp)
library(knitr)
library(pracma)

#Set working directory
setwd("C:/Users/qdani/OneDrive/Documents/grad_school/coursework/ECON8210/homework/hw1/rcpp_test/")

#load Cpp functions
sourceCpp("numerical_analysis.cpp")

set.seed(2024)


```

## 1. Github Repository

My code can be found in the [github repository located here](https://github.com/qdanielson/econ8210.homework).

## 2. Quadratures

```{r quadratures, Echo = FALSE}
# Set parameters
t <- 100
rho <- .04
lambda <- .02

# Quadrature methods
# Set up output matrix, precisions

precision_list <- c(10,1,.1,.01,.001)

output_mat <- matrix(nrow = 4,ncol = length(precision_list))
time_mat <- matrix(nrow = 4,ncol = length(precision_list))

colnames(output_mat) <- precision_list
colnames(time_mat) <- precision_list

rownames(output_mat) <- c("Midpoint","Trapezoid","Simpson","Monte Carlo")
rownames(time_mat) <- c("Midpoint","Trapezoid","Simpson","Monte Carlo")

# Define R functions for MC analysis which doesn't use Rcpp

# Write functions for integral

gR <- function(x){exp(-rho*x)}
fR <- function(x){1 - exp(-lambda*x)}
uR <- function(x){-exp(-1*x)}

int <- function(x){gR(x)*uR(fR(x))}

# Begin loop over precisions

for (precision in precision_list) {
  
  t_seq <- seq(0,t,by=precision)
  
  mid_start <- Sys.time()
  
  V_midpoint <- midpoint_rule(t_seq,rho,lambda)
  
  mid_end <- Sys.time()
  
  # Trapezoid

  trap_start <- Sys.time()
  
  V_trapezoid <- trapezoid_rule(t_seq,rho,lambda)
  
  trap_end <- Sys.time()
  
  # Simpson Rule
  
  sim_start <- Sys.time()
  
  V_simpson <- trapezoid_rule(t_seq,rho,lambda)
  
  sim_end <- Sys.time()
  
  # Monte Carlo
  
  MC_start <- Sys.time()
  
  draws <- runif(100/precision,0,100)
  
  V_MC <- 100*mean(int(draws))
  
  MC_end <- Sys.time()
  
  outputV <- c(V_midpoint,V_trapezoid,V_simpson,V_MC)
  
  outputT <- c(mid_end - mid_start, trap_end - trap_start, sim_end - sim_start, MC_end - MC_start)
  
  output_mat[,which(precision == precision_list)] <- outputV
  time_mat[,which(precision == precision_list)] <- outputT
  
  rm(outputV)
  rm(outputT)
  
}
kable(output_mat,caption = "Value of Integral by Method, Precision")
kable(time_mat,caption = "Time to Calculate by Method, Precision")
```
The three quadrature methods perform about equally well -- notably, the midpoint rule overestimates the sum a bit while trapezoid and Simpson rules underestimate it. However, once I move from considering 10 points to 100 points, all three perform about equally well. The Monte Carlo method does not perform particularly well until I sample 10,000 points at the highest level of precision.

However, Monte Carlo performs somewhere between one hundred and 50 percent faster than a quadrature method at the same precision. This suggests that the technique is a good choice for computationally taxing problems.

## 3. Optimization

```{r Optimization, echo=FALSE}
# Define x_vec, y_vec, and epsilon in R
precision <- .001
epsilon <- .000001

x_vec <- seq(-1, 3, by = precision)
y_vec <- seq(-1, 3, by = precision)

start <- Sys.time()

# Call the C++ function
result <- derivative_and_hessian(x_vec, y_vec, epsilon)

end <- Sys.time()

paste0("Derivatives and Hessian calculated in ",round(as.numeric(end - start),3)," seconds")

# Access results
V_mat <- result$V_mat
dV_array_x <- result$dV_array_x
dV_array_y <- result$dV_array_y
hessian_xx <- result$hessian_xx
hessian_xy <- result$hessian_xy
hessian_yx <- result$hessian_yx
hessian_yy <- result$hessian_yy

dV_array <- array(dim = c(dim(dV_array_x)[1], dim(dV_array_x)[2], 2)) # derivative array
dV_array[,,1] <- dV_array_x
dV_array[,,2] <- dV_array_y

HD_array <- array(dim = c(dim(hessian_xx)[1], dim(hessian_xx)[2], 2, 2)) # Hessian array
HD_array[,,1,1] <- hessian_xx
HD_array[,,1,2] <- hessian_xy
HD_array[,,2,1] <- hessian_yx
HD_array[,,2,2] <- hessian_yy

# Gradient descent

l <- length(x_vec)

alpha <- 1

xstart <- sample(1:length(x_vec),1)
ystart <- sample(1:length(y_vec),1)

point_mat <- matrix(nrow = 10000, ncol = 2)
value_mat <- matrix(nrow = 10000, ncol = 2)

point_mat[1,] <- c(xstart,ystart)

err <- sum(abs(point_mat[1,] - c(2001,2001)))

start <- Sys.time()

i <- 1

while (err > 1) {
  
  i <- i + 1
  
  point <- c(x_vec[point_mat[i-1,1]],y_vec[point_mat[i-1,2]])
  
  value_mat[i-1,] <- point
  
  d <- -(dV_array[point_mat[i-1,1],point_mat[i-1,2],]/sqrt(sum(dV_array[point_mat[i-1,1],point_mat[i-1,2],]^2)))
  
  movement_r <-  d*(10*alpha/i)
  movement_p <-  round(movement_r/precision,0)
  
  point_mat[i,] <- point_mat[i-1,] + movement_p
  
  # Make sure we are in-bounds
  point_mat[i,1] <- min(point_mat[i,1],l)
  point_mat[i,1] <- max(point_mat[i,1],1)
  
  point_mat[i,2] <- min(point_mat[i,2],l)
  point_mat[i,2] <- max(point_mat[i,2],1)
  
  err <- sum(abs(point_mat[i,] - c(2001,2001)))
}

end <- Sys.time()

t_grad <- end - start

l_grad <- i


# Conjugate Descent

alpha <- 1
beta <- 0
d_lag <- matrix(c(0,0),nrow = 2, ncol = 1)

xstart <- sample(1:length(x_vec),1)
ystart <- sample(1:length(y_vec),1)

point_mat <- matrix(nrow = 1000, ncol = 2)
value_mat <- matrix(nrow = 1000, ncol = 2)

point_mat[1,] <- c(xstart,ystart)

err <- sum(abs(point_mat[1,] - c(2001,2001)))

i <- 1

start <- Sys.time()

while (err > 1) {
  
  i <- i + 1
  
  point <- c(x_vec[point_mat[i-1,1]],y_vec[point_mat[i-1,2]])
  
  value_mat[i-1,] <- point
  
  d <- -(dV_array[point_mat[i-1,1],point_mat[i-1,2],]) + beta[[1]] * d_lag
  
  movement_r <-  d/sqrt(sum(d^2))*(alpha/i)
  movement_p <-  round(movement_r/precision,0)
  
  point_mat[i,] <- point_mat[i-1,] + movement_p
  
  # Make sure we are in-bounds
  point_mat[i,1] <- min(point_mat[i,1],l)
  point_mat[i,1] <- max(point_mat[i,1],1)
  
  point_mat[i,2] <- min(point_mat[i,2],l)
  point_mat[i,2] <- max(point_mat[i,2],1)
  
  d_lead <- -(dV_array[point_mat[i,1],point_mat[i,2],])
  
  beta <- (t(d_lead)%*%d_lead)/(t(d)%*%d)
  
  d_lag <- d
  
  rm(d)
  rm(d_lead)
  
  err <- sum(abs(point_mat[i,] - c(2001,2001)))
  
}

end <- Sys.time()

t_conj <- end - start

l_conj <- i

# Newton-Raphson

xstart <- sample(1:length(x_vec),1)
ystart <- sample(1:length(y_vec),1)

point_mat <- matrix(nrow = 1000, ncol = 2)
value_mat <- matrix(nrow = 1000, ncol = 2)

point_mat[1,] <- c(xstart,ystart)

err <- sum(abs(point_mat[1,] - c(2001,2001)))

start <- Sys.time()

i <- 1

while (err > 1) {
  
  i <- i + 1
  
  point <- c(x_vec[point_mat[i-1,1]],y_vec[point_mat[i-1,2]])
  
  value_mat[i-1,] <- point
  
  H <- HD_array[point_mat[i-1,1],point_mat[i-1,2],,]
  
  d <- solve(H) %*% dV_array[point_mat[i-1,1],point_mat[i-1,2],]
  
  movement_r <-  -d
  movement_p <-  round(movement_r/precision,0)
  
  point_mat[i,] <- point_mat[i-1,] + movement_p
  
  # Make sure we are in-bounds
  point_mat[i,1] <- min(point_mat[i,1],l)
  point_mat[i,1] <- max(point_mat[i,1],1)
  
  point_mat[i,2] <- min(point_mat[i,2],l)
  point_mat[i,2] <- max(point_mat[i,2],1)
  
  rm(d)
  
  err <- sum(abs(point_mat[i,] - c(2001,2001)))
  
}

end <- Sys.time()

t_nr <- end - start

l_nr <- i

# BFGS

xstart <- sample(1:length(x_vec),1)
ystart <- sample(1:length(y_vec),1)

point_mat <- matrix(nrow = 1000, ncol = 2)
value_mat <- matrix(nrow = 1000, ncol = 2)

point_mat[1,] <- c(xstart,ystart)

err <- sum(abs(point_mat[1,] - c(2001,2001)))

H <- HD_array[point_mat[1,1],point_mat[1,2],,]
H_inv <- solve(H)

start <- Sys.time()

i <- 1

while (err > 1) {
  
  i <- i + 1
  
  point <- c(x_vec[point_mat[i-1,1]],y_vec[point_mat[i-1,2]])
  
  value_mat[i-1,] <- point
  
  d <- H_inv %*% dV_array[point_mat[i-1,1],point_mat[i-1,2],]
  
  movement_r <-  -d
  movement_p <-  round(movement_r/precision,0)
  
  if (sum(movement_p == c(0,0)) == 2) {            # Adjust point slightly so it does not get stuck
    movement_p[,1] <- sample(c(-1,1),2)
      }
  
  point_mat[i,] <- point_mat[i-1,] + movement_p
  
  # Make sure we are in-bounds
  point_mat[i,1] <- min(point_mat[i,1],l)
  point_mat[i,1] <- max(point_mat[i,1],1)
  
  point_mat[i,2] <- min(point_mat[i,2],l)
  point_mat[i,2] <- max(point_mat[i,2],1)
  
  new_point <- c(x_vec[point_mat[i,1]],y_vec[point_mat[i,2]])
  
  s <- new_point - point
  g <- dV_array[point_mat[i,1],point_mat[i,2],] - dV_array[point_mat[i-1,1],point_mat[i-1,2],]
  
  H_inv_new <- (diag(2) - (s %*% t(g))/(t(g) %*% s)[[1]]) %*% H_inv %*% (diag(2) - (g %*% t(s))/(t(g) %*% s)[[1]]) + ((s %*% t(s)))/((t(g) %*% s)[[1]])
  
  H_inv <- H_inv_new
  
  if(sum((point - new_point) == c(0,0)) == 2) {    # If stuck at edge or corner, pick new point
    
    point_mat[i,] <- c(sample(1:length(x_vec),1),
                       sample(1:length(y_vec),1))
    
    H <- HD_array[point_mat[i,1],point_mat[i,2],,]
    H_inv <- solve(H)
    
  }
  
  rm(d)
  rm(H_inv_new)
  rm(s)
  rm(g)
  
  err <- sum(abs(point_mat[i,] - c(2001,2001)))
  
}

end <- Sys.time()

t_bfgs <- end - start

l_bfgs <- i

output_opt <- matrix(c(l_grad,l_conj,l_nr,l_bfgs,t_grad,t_conj,t_nr,t_bfgs),nrow = 4,ncol = 2)

colnames(output_opt) <- c("Iterations","Time")
rownames(output_opt) <- c("Gradient Descent","Conjugate Descent","Newton-Raphson","BFGS")

kable(output_opt)

```

I calculate numerical derivatives with respect to the Rosenbock Function for x and y numerically, and also calculate the Hessian matrix for this function from the numerical derivatives. I use Rcpp to accelerate these calculations and am able to calculate all six matrices across the whole grid in about two seconds.

The four methods are sorted in the table in order of improving performance. Gradient descent, while computationally very simple, takes a large number of iterations to converge to the true value. This is in part because I set the step size to be both small and decreasing. In contrast, conjugate descent is much faster, which makes sense for this function (Rosenbrock Function), which consists of a U-shaped valley where it is easy for an optimization algorithm to get stuck (It happened a lot while I was coding this question!).

The two Hessian methods are comparatively much faster, converging is less than 1 percent of the iternations needed by gradient descent. For these two I had issues with the boundary values, as the Hessian matrix terms implied large moves across the grid. To fix this, I hard coded in adjustments to return to a point on the boundary. The BFGS is even faster, in terms of both iterations and total time. However, I found it to be very sensitive to the initial point selected. After iterating a good bit I got it to work almost all the time, though it is far less reliable than Newton-Raphson.

## 4, 5 Allocation Problem

```{r allocation, Echo = FALSE}
## Problem 3 - Grid Search Optimization

startg <- Sys.time()

# Initialize grid

precision <- .0001
split <- seq(0,1,precision)

create_params <- function(agent_count,
                          benchmark) {

  if (benchmark == TRUE) {
    
    alpha <- rep(1,agent_count)
    lambda <- rep(1,agent_count)
    omega <- matrix(rep(-.5,agent_count*agent_count),nrow = agent_count)
    
  } else {
    
    alpha <- runif(agent_count) + 1
    lambda <- runif(agent_count) + 1
    omega <- matrix(-(runif(agent_count*agent_count) + .5),nrow = agent_count)
    
  }
  
  return(list(alpha = alpha,lambda = lambda,omega = omega))
}

start <- Sys.time()

e_mat <- matrix(1,nrow = 3,ncol = 3)

x_mat.e <- matrix(ncol = 3, nrow = 3)
x_mat.v <- matrix(ncol = 3, nrow = 3)

params_even <- create_params(agent_count = 3, benchmark = TRUE)

params_vary <- create_params(agent_count = 3, benchmark = FALSE)

alpha_even <- params_even$alpha
omega_even <- params_even$omega
lambda_even <- params_even$lambda

alpha_vary <- c(.9,1,1.1)
omega_vary <- -matrix(c(.4,.5,.6,
                        .6,.5,.4,
                        .5,.4,.6),
                      nrow = 3,
                      ncol = 3,
                      byrow = TRUE)
lambda_vary <- c(.9,1,1.1)

for (i in 0:2) {
  
  output <- grid_search(e_mat,alpha = alpha_even,omega = omega_even,lambda = lambda_even,split = split,index = i)
  
  max.row <- output[which(output[,3] == max(output[,3])),]
  
  e <- sum(e_mat[,i+1])
  
  x.col <- c(e * max.row[1],
            (e - (e * max.row[1])) * max.row[2],
            (e - (e * max.row[1])) * (1 - max.row[2]))
  
  x_mat.e[,i+1] <- x.col
  
}

colnames(x_mat.e) <- paste0("alpha = ",alpha_even)
rownames(x_mat.e) <- paste0("lambda = ",lambda_even)

for (i in 0:2) {
  
  output <- grid_search(e_mat,alpha = alpha_vary,omega = omega_vary,lambda = lambda_vary,split = split,index = i)
  
  max.row <- output[which(output[,3] == max(output[,3])),]
  
  e <- sum(e_mat[,i+1])
  
  x.col <- c(e * max.row[1],
             (e - (e * max.row[1])) * max.row[2],
             (e - (e * max.row[1])) * (1 - max.row[2]))
  
  x_mat.v[,i+1] <- x.col
  
}

colnames(x_mat.v) <- paste0("alpha = ",alpha_vary)
rownames(x_mat.v) <- paste0("lambda = ",lambda_vary)

colnames(omega_vary) <- paste0("good ",1:3)
rownames(omega_vary) <- paste0("agent ",1:3)

endg <- Sys.time()

kable(x_mat.e, caption = "Allocation of good to agent with even parameters")

kable(x_mat.v, caption = "Allocation of good to agent with heterogenous parameters")

kable(omega_vary, caption = "Omega values by agent, good")


paste0("Grid searches performed in ",round(as.numeric(endg - startg),3)," seconds")

```

I used a grid search algorithm for this problem--It successfully calculates Pareto-optimal distributions for both cases. The columns represent each of the three goods, and rows represent agents. I also report the heterogenous values of omega used in the allocation. Each omega value corresponds to the same cell in the heterogeneous agent parameters table.

The grid search algorithm works, but it is very slow (compared to descent methods) and requires a lot of memory, especially in higher dimensions. For example, a 10-dimensional grid with 10 grid points in each dimension would require around 75 GB of free memory.

``` {r allocation2, echo = TRUE}

# Demonstrate why brute force grid search will not work for 10 agent-10 good case

V3 <- array(data = runif(10^3),dim = rep(10,3))

object.size(V3)

# V10 <- array(data = runif(10^10),dim = rep(10,10)) 
print("Error: cannot allocate vector of size 74.5 Gb")
```

One way to do it would be to nest loops and solve "recursively", that is, nest 9 loops (agents - 1, since the consumption of the last agent is pinned down by the other 9) and within each loop keep only the maximum value. This ensures that you only have to keep a vector of size (10001 in the 3-agent case) at all times. But this would require very careful coding to avoid memory issues and still requires 10001^9 calculations. An alternative approach involving solving the analytic derivatives by hand and doing gradient or conjugate descent, as this would allow you to evaluate the function only at the necessary points.

``` {r allocation3}
## 4 - Solve competitive equilibrium

# a * (x^1+om)/1+om -> 1/a * (p*theta)^(1/om) = x

alpha_mat <- matrix(alpha_even,3,3,byrow = T)

x_mat.e <- round(x_mat.e,3)
x_mat.v <- round(x_mat.v,3)

f.even <- function(point) {
  
  x <- x_mat.e # values from x_mat
  
  alpha_mat <- matrix(alpha_even,3,3,byrow = T) # Values from alpha
  
  e_mat <- matrix(1,3,3) # values set
  
  omega <- omega_even
  
  p <- c(1,point[1:2]) # normalize price 1 to 1
  
  theta <- point[3:5]
  
  ptheta <- p %*% t(theta)
  
  x.guess <- (1/alpha_mat) * ptheta^omega
  
  x.error <- colSums(x - x.guess)
  
  p.error <- (e_mat %*% p) - (x.guess %*% p)
  
  return(c(x.error,p.error))
  
}

f.vary <- function(point) {
  
  x <- x_mat.v # values from x_mat
  
  alpha_mat <- matrix(alpha_vary,3,3,byrow = T) # Values from alpha
  
  e_mat <- matrix(1,3,3) # values set
  
  omega <- omega_vary
  
  p <- c(1,point[1:2]) # normalize price 1 to 1
  
  theta <- point[3:5]
  
  ptheta <- p %*% t(theta)
  
  x.guess <- (1/alpha_mat) * ptheta^omega
  
  x.error <- colSums(x - x.guess)
  
  p.error <- (e_mat %*% p) - (x.guess %*% p)
  
  return(c(x.error,p.error))
  
}

output1 <- fsolve(f.even,c(1.1,   # price 2
                1.1,   # price 3
                1.1,   # theta 1
                1.1,   # theta 2
                1.1))  # theta 3

output2 <- fsolve(f.vary,c(1,   # price 2
                1,   # price 3
                1,   # theta 1
                1,   # theta 2
                1))  # theta 3

output1_text <- paste(
  "Prices for each good, even parameters:",
  paste("p1:", 1),
  paste("p2:", output1$x[1]),
  paste("p3:", output1$x[1]),
  sep = "\n"
)

output2_text <- paste(
  "Prices for each good, even parameters:",
  paste("p1:", 1),
  paste("p2:", round(output2$x[1],3)),
  paste("p3:", round(output2$x[2],3)),
  sep = "\n"
)

cat(output1_text)

cat(output2_text)


```

I did not use a grid search to solve the nonlinear equations imlied by the first order equations. Instead, I applied the function "fsolve" from R package "pracma", which ports many populat MATLAB functions to R. This implementations is quick and calculates even prices in the case of even parameters and uneven prices in the other case. Since my parameters are not too crazy, these prices are still near 1.

## 6. Value Function Iteration

### a. Social Planner's Problem

The social planner's problem can be expressed recursively as:

$$V(k,i_{-1},z,\tau) = \max_{c,k',i}[log(c) + \gamma log(g) - \frac{l^2}{2} + \beta \mathbb{E}[V(k',i,z',\tau')] $$

Such that:

$$c+i=(1-\tau)wl+rk $$
$$c+i+g=e^zk^\alpha l^{1-\alpha}$$
$$i=k'-(1-\delta)k+\Phi(i,i_{-1})$$
$$\Phi(i,i_{-1})=\phi(\frac{i}{i_{-1}} - 1)^2 i$$
$$g=\tau w l$$

For the steady-state, the first order conditions of the objective function with respect to capital and labor: (Drop the c' for c in steady-state)

$$\frac{1}{c}=\beta \mathbb{E}[\frac{1}{c'}(r'+1+\delta)]$$
$$l=\frac{1}{c}(1-\tau)w$$

And wage and rate are pinned down by the first order conditions of the firm:

$$r=\alpha e^z k^{\alpha - 1} l^{1-\alpha}$$
$$w=(1-\alpha) e^z k^{\alpha} l^{-\alpha}$$

And the control equations are:

$$g=\tau w l$$
$$c+i+g=e^zk^\alpha l^{1-\alpha}$$
$$k=\frac{1}{\delta}i$$
### b. Steady-state

Below is code I used to calculate steady-state values for this model. It is calculated again using fsolve from pracma:

```{r steadystate, echo=TRUE}

## Question 6 -- VFI

# Parameters
params <- c(aalpha = 0.33, bbeta = 0.97, ddelta = 0.1, pphi = 0.05, ggamma = 0.2)
aalpha <- params["aalpha"]
bbeta <- params["bbeta"]
ddelta <- params["ddelta"]
pphi <- params["pphi"]
ggamma <- params["ggamma"]
ttau <- 0.25
z <- 0.0

# Define functions for control variables 

rate <- function(aalpha,z,k,l){
  r <- aalpha * exp(z) * k^(aalpha - 1) * l^(1 - aalpha)
  return(r)
}

wage <- function(aalpha,z,k,l){
  w <- (1 - aalpha) * exp(z) * k^(aalpha) * l^(-aalpha)
  return(w)
}

govt <- function(ttau,w,l){
  g <- ttau * w * l
  return(g)
}

consumption <- function(ttau,w,r,k,l,i){
  c <- (1 - ttau) * w * l + r * k - i
  return(c)
}

inv_ss <- function(k,ddelta){ # (STEADY STATE VERSION)
  i <- k * ddelta
  return(i)
}

adjustment.costs <- function(pphi,il,i){
  ac <- pphi * ((i/il) - 1)^2 * i
  return(ac)
}

k.next <- function(ddelta,k,i,ac){
  k1 <- i + (1 - ddelta) * k - ac
  return(k1)
}

## Define and solve steady-state

sseq <- function(vars){
  
  z <- 0
  ttau <- .25
  aalpha <- 1/3
  ddelta <- .1
  bbeta <- .97
  
  # unpack vars
  k <- vars[1]
  l <- vars[2]
  
  # Calculate control variables
  r <- rate(aalpha,z,k,l)
  w <- wage(aalpha,z,k,l)
  i <- inv_ss(k,ddelta)
  c <- consumption(ttau,w,r,k,l,i)
  
  # FOC errors (objective function wrt. k, l)
  e1 <- 1 - bbeta * (r + 1 - ddelta)
  e2 <- (1/c) * (1 - ttau) * w - l
  
  return(c(e1,e2))
}

ss.soln <- fsolve(sseq,c(1,1))$x

k.ss <- ss.soln[1]
i.ss <- ddelta * k.ss
l.ss <- ss.soln[2]
r.ss <- rate(aalpha,z = 0,k.ss,l.ss)
w.ss <- wage(aalpha,z = 0,k.ss,l.ss)
c.ss <- consumption(ttau = .25,w.ss,r.ss,k.ss,l.ss,i.ss)
g.ss <- govt(ttau = .25,w.ss,l.ss)

V.ss <- log(c.ss) + ggamma * log(g.ss) - (1/2) * l.ss^2

```

```{r print, echo = FALSE}
print(paste0("Steady-state capital: ",round(k.ss,3)))
print(paste0("Steady-state investment: ",round(i.ss,3)))
print(paste0("Steady-state labor: ",round(l.ss,3)))
print(paste0("Steady-state rental rate: ",round(r.ss,3)))
print(paste0("Steady-state wage: ",round(w.ss,3)))
print(paste0("Steady-state consumption: ",round(c.ss,3)))
print(paste0("Steady-state government consumption: ",round(g.ss,3)))
print(paste0("Steady-state Value: ",round(V.ss,3)))
```

### c. value function iteration

```{r bellmansetup, echo = FALSE}

make.ttau.z.transition.matrix <- function(){
  
  ttau.levels <- 1:3
  z.levels <- 1:5
  
  ttau.grid <- c(0.2,0.25,0.3)
  z.grid <- c(-0.0673,-0.0336,0,0.0336,0.0673)
  
  ttau.probabilities <- matrix(c(0.9,0.1,0,
                                 0.05,0.9,0.05,
                                 0,0.1,0.9),
                               nrow = 3,
                               ncol = 3,
                               byrow = TRUE)
  
  z.probabilities <- matrix(c(0.9727,0.0273,0,0,0,
                              0.0041,0.9806,0.0153,0,0,
                              0,0.0082,0.9836,0.0082,0,
                              0,0,0.0153,0.9806,0.0041,
                              0,0,0,0.0273,0.9727),
                            nrow = 5,
                            ncol = 5,
                            byrow = TRUE)
  
  ttau.z.mapping <- as.matrix(expand.grid(ttau = ttau.levels,z = z.levels))
  
  ll <- nrow(ttau.z.mapping)
  
  ttau.z.transition.matrix <- matrix(NA,
                                     nrow = ll,
                                     ncol = ll)
  for (i in 1:ll) {
    for (j in 1:ll) {
      
      ttau.z.transition.matrix[i,j] <- ttau.probabilities[ttau.z.mapping[i,1],ttau.z.mapping[j,1]] * z.probabilities[ttau.z.mapping[i,2],ttau.z.mapping[j,2]]
      
    }
  }
  
  return(list(ttau.z.mapping,ttau.z.transition.matrix))
}

ttau.z.transition.mapping <- make.ttau.z.transition.matrix()[[1]]
ttau.z.transition.matrix <- make.ttau.z.transition.matrix()[[2]]

set.params <- function(){
  # Parameters for the function
  aalpha <- 0.33
  bbeta <- 0.97
  ddelta <- 0.1
  pphi <- .05
  ggamma <- .2
  
  return(c(aalpha,
           bbeta,
           ddelta,
           pphi,
           ggamma))
}

params <- set.params()

k.grid <- seq(.7 * k.ss, 1.3 * k.ss, length.out = 25)
il.grid <- seq(.5 * i.ss, 1.5 * i.ss, length.out = 5)

V0 <- array(0, dim = c(25,5,15))

V <- V0

```


I wrote the following function for the bellman operator. It takes an initial guess for the value function, state variables k and lagged i, function parameters, and transition matrices. I have combined both matrices into a single composite state. The Bellman gives accurate values at the steady state, suggesting that it is coded correctly. Performing value functon iteration in R was computationally costly and slow, so I attempted to code the loop in C++ using libraries like gsl and nlopt. I was not able to code something that worked before the assignment deadline

```{r bellman, echo = TRUE}

bellman <- function(opt.vars, state.vars, params, V, ttau.z.transition.mapping, ttau.z.transition.matrix) {
  
  ttau.list <- c(0.2,0.25,0.3)
  z.list <- c(-0.0673,-0.0336,0,0.0336,0.0673)
  
  # Unpack parameters
  aalpha <- params[1]
  bbeta <- params[2]
  ddelta <- params[3]
  pphi <- params[4]
  ggamma <- params[5]
  
  # Unpack state variables
  k <- state.vars[1]
  il <- state.vars[2]
  ttau <- .25 #ttau.list[ ttau.z.transition.mapping[state.vars[3], 1]]
  z <-  0 #z.list[ttau.z.transition.mapping[state.vars[3], 2]]
  
  # Unpack "opt.vars"
  l <- opt.vars[1]
  i <- opt.vars[2]
  
  # Calculate control variables
  r <- rate(aalpha, z, k, l)
  w <- wage(aalpha, z, k, l)
  c <- consumption(ttau, w, r, k, l, i)
  g <- govt(ttau, w, l)
  ac <- adjustment.costs(pphi, il, i)
  k1 <- k.next(ddelta, k, i, ac)
  
    # Keep k1 and i within grid range to avoid NaN in interpolation
  k1 <- max(min(k1, max(k.grid)), min(k.grid))

  # Calculate expected future value with interpolation
  V.next <- vector(length = 15L)
  for (state in 1:15) {
    V.next[state] <- interp2(x = il.grid, y = k.grid, V[,,state], xp = i, yp = k1)
  }
  
  # Calculate EV
  EV <- V.next %*% ttau.z.transition.matrix[1, ]
  
  # Compute the value function
  V <- log(c) + ggamma * log(g) - (1 / 2) * l^2 + EV
  
  return(V)
}

test.V <- bellman(c(l.ss,i.ss),c(k.ss,i.ss,.25,0),params,V, ttau.z.transition.mapping,ttau.z.transition.matrix)

print(test.V)


```

